---
title: "Arts Education Research Methods Summit: Intro to R"
output: learnr::tutorial
runtime: shiny_prerendered
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
library(learnr)
library(AERMS)
library(tidyverse)
library(knitr)
library(rstatix)
library(QuantPsyc)
knitr::opts_chunk$set(echo = TRUE)
options(tutorial.storage = list(
  save_object = function(...) { },
  get_object = function(...) NULL,
  get_objects = function(...) list(),
  remove_all_objects = function(...) { }
))
```

## Introduction

This is a series of tutorial exercises to familiarize yourself with the
basics of using [R](https://www.r-project.org/) when working within the
[R Studio environment](https://posit.co/download/rstudio-desktop/).

R is free, open-source software and a programming language that can be
used for data analysis, visualization, and more. One of the benefits of
an open-source software package is that anyone with the requisite
programming skill can contribute to its development and it can
continually adapt to emerging needs in the field. Contributors to the
open-source R project can create **packages** that, once installed, will
provide you with virtually any statistical analysis tool you could ever
need as an arts education researcher.

The programming language can appear opaque at first, but a combination
of (a) a little knowledge, (b) practice, and (c) strategic "Googling"
will eventually allow you to use R for a wide variety of tasks fairly
quickly.

## Creating an R Project to Organize your Data Analysis Files

### *R Projects*

The "learning curve" for working with R can at first be fairly steep.
Getting started in the R Studio environment, organizing and accessing
files, and even loading a data set for analysis can be tricky obstacles
in and of themselves.

Working with **R Projects** in R Studio will help smooth out some of the
bumps in the road. Starting an R Project will automatically group all of
your files in a single folder/directory and help you avoid problems
finding files in R.

Setting one up involves the following navigation:

-   File -\> "New Project..."
-   Then choose to house your R Project in either a "New Directory"
    (i.e., a new folder on your computer) or an "Existing Directory"
    (i.e., a folder that already exists on your computer)

Once you create your R Project directory, you will see that the lower
right panel of R Studio shows the directory address you created (or
chose) and all of the files in that directory. You should save any data
files you need for your analyses in the same directory. If you do, it
will be easier to load your data and use it later on.

If you don't work from an R Project, you will have to manage the
directory designations for your files manually ... and that can cause
errors and frustration.

### *R Script*

An **R script** file is where you will create and save the code you
write for your projects.

Opening an R script file involves the following navigation:

-   File -\> "New File" -\> "R Script"

If you are working in an R Project when you create your R script file,
the new script file will automatically be organized (i.e., saved) with
the rest of your project files.

## Basic Elements of the R Programming Language

### *Calculator Computations*

R can be used as a calculator by entering numbers along with the typical
operators:

-   Addition: +
-   Subtraction: -
-   Multiplication: \*
-   Division: /
-   Exponentiation: \^
-   You can also use parentheses () for complex operations

This is a convenient feature in R that will be useful should you need to
transform data or conduct manual computations.

Use the code space below to experiment with some calculator functions,
click "**Run Code**" in the upper right to see the output/results:

```{r calculator, exercise = TRUE, exercise.eval = FALSE}

```

### *Commenting in Code*

It's usually a good idea to make notes while coding so that you can so it will be easier to remember what your code is trying to accomplish at a later
date. You can create commented code by using the number sign symbol: `#`.

Notice how the "commented" bits are in a different color in the following code as well as how only the non-commented bits are actually run by R.

```{r commenting, exercise = TRUE, exercise.eval = FALSE}
# This is a comment - it will essentially be ignored by R

4 + 4

# Notice also how this commented operation is ignored by R, that is, not run
# 4 + 4

# The only thing that will result from all of this code is the answer to 4 + 4
```

### *Data Types*

It is important to recognize the **type** of variable you are using when
working in R since it will have major implications for the sorts of
analyses you can conduct as well as  whether you will be able to execute your
code without errors.

There are four levels of measurement commonly described in statistics:

-   Interval and Ratio (referred to as "scalar" in SPSS)
-   Ordinal
-   Nominal

R recognizes similar types of data, but has slightly different
distinctions and uses different terminology:

-   Numeric - continuous quantitative values (10.5, 55.3, 787.0)
-   Integer - discrete quantiative values with no decimal values (1, 55, 100)
-   Factor - discrete categories with each category called a "level" (an analog to a nominal variable) ("cat", "dog", "fish")
-   Ordered factor - discrete categories that are ordered (analogous to an ordinal variable) ("never", "sometimes", "always")

R also recognizes three additional data types:

-   Character - letters with no additional information attached ("k", "R
    is exciting", "FALSE") (referred to as "string" in SPSS)
-   Logical - TRUE or FALSE Boolean values
-   Complex - for the use of imaginary numbers, indicated by "i"

## Assigning "Objects" and "Storing" Information in the Global Environment

R can store objects in its **global environment**, that is, it's memory.
You can see any objects currently in the global environment in the upper
right panel of R Studio under the tab titled "Environment."

Storing objects is simply a matter of assigning information to a name
and you can do that using a `<-` symbol. You can then call the name in R
or perform operations with the object and the information will be
displayed as appropriate. See the following examples in the code below:

```{r assign_variables, exercise = TRUE, exercise.eval = FALSE}
# The lines showing the assignments of numbers to x and y will not result in any 
# output. However, R will have stored the values under those designated assignments
# in the R Environment.
x <- 4
y <- 4

# Each of the following lines will produce output.
x
y
x + y

# This line will not produce output, it will only store the sum of the information
# in x and y in "z" in the R Environment.
z <- (x + y)

# This line will produce output.
z/2
```

Many things are typically stored as objects when using R, including:
variables, data sets (and any modified data sets you create), outputs of
analyses, and custom built functions.

### *Vectors*

You can also assign a collection of values, i.e., a **vector** of
values, to an object using the **concatenate** function `c()` as long as you
list the values separated by commas. The following code shows the
storage of variables of three different data types: *numeric*, *integer*, and
*character*. Note also that you will have to use parentheses around the values when including character values in a vector.

R will do a pretty good job at guessing what type of data the values in
each variable represent when the values are numbers. But, R will
typically interpret any vectors that contain letters or words as
character variables unless directed otherwise.

```{r assign_vectors, exercise = TRUE, exercise.eval = FALSE}
# As with the code above, the lines showing the assignments of vectors to each 
# name will not result in any output. However, R will have stored the vectors 
# under those designated assignments.
numeric_variable <- c(1.2, 3.7, 5, 2.2, 4.69, 6.123, 10.1, 4.3, 5.5, 6.09)
integer_variable <- c(4, 8, 15, 16, 23, 42, 12, 34, 78, 2)
character_variable <- c("who", "what", "when", "who", "what", "when", "who", 
                        "what",  "who", "what")

# Calling the name of each of the vectors will produce output.
numeric_variable
integer_variable
character_variable
```

As with single value assignments to objects, R will also allow you to
perform operations with stored vectors (or really any kind of data type).
See the following as an example:

```{r operate_vectors, exercise = TRUE, exercise.eval = FALSE}
# First create the example variables.
numeric_variable <- c(1.2, 3.7, 5, 2.2, 4.69, 6.123, 10.1, 4.3, 5.5, 6.09)
integer_variable <- c(4, 8, 15, 16, 23, 42, 12, 34, 78, 2)

# Then perform vector-based operations
numeric_variable*2
integer_variable/2
```

### *Dataframes*

R can store and operate upon all sorts of data objects (values,
vectors, matrices, and so on). An object that serves as the basis for
many statistical analyses is the **dataframe**. A dataframe is R's term
for what most would simply call a data set: an arrangement of data with
variables as columns and entries as rows.

You can create dataframes by assembling vectors of the same length into
a single object using the `data.frame()` function. An example of doing
this is provided below.

```{r assign_dataframe, exercise = TRUE, exercise.eval = FALSE}
# First, we store the variables under designated assignments.
numeric_variable <- c(1.2, 3.7, 5, 2.2, 4.69, 6.123, 10.1, 4.3, 5.5, 6.09)
integer_variable <- c(4, 8, 15, 16, 23, 42, 12, 34, 78, 2)
character_variable <- c("who", "what", "when", "who", "what", "when", "who", 
                        "what",  "who", "what")

# Then you can bind them together into a dataset called "my_data" with three 
# columns (i.e., variables) and 10 rows (i.e., entries).
my_data <- data.frame(numeric_variable, integer_variable, character_variable)

# Calling the "my_data" object shows the entire dataset
my_data
```

You can also examine a data frame using a variety of different *viewing*
functions. See below how the `head()` function returns the first six
rows, the `tail()` function returns the last six rows, and the `str()` function displays the structure of the data.

```{r view_dataframe, exercise = TRUE, exercise.eval = FALSE}
# First, we store the variables under designated assignments.
numeric_variable <- c(1.2, 3.7, 5, 2.2, 4.69, 6.123, 10.1, 4.3, 5.5, 6.09)
integer_variable <- c(4, 8, 15, 16, 23, 42, 12, 34, 78, 2)
character_variable <- c("who", "what", "when", "who", "what", "when", "who", 
                        "what",  "who", "what")

# Then you can bind them together into a dataset called "my_data" with three 
# columns (i.e., variables) and 10 rows (i.e., entries).
my_data <- data.frame(numeric_variable, integer_variable, character_variable)

head(my_data)
tail(my_data)
str(my_data)
```

To view a single variable from a dataframe you will need to use an
additional symbol to specify to R that it should search in the
particular dataframe the variable is housed in, the `$` after the name of the dataframe, then the name of the variable as in: `dataframe_name$variable_name`.

```{r view_dataframe_select, exercise = TRUE, exercise.eval = FALSE}
# Again, these first lines are just creating the dataframe for this example.
numeric_variable <- c(1.2, 3.7, 5, 2.2, 4.69, 6.123, 10.1, 4.3, 5.5, 6.09)
integer_variable <- c(4, 8, 15, 16, 23, 42, 12, 34, 78, 2)
character_variable <- c("who", "what", "when", "who", "what", "when", "who", 
                        "what",  "who", "what")
my_data <- data.frame(numeric_variable, integer_variable, character_variable)

# Use the "$" to call a single variable within a dataframe
my_data$numeric_variable
```

You may have noticed earlier that when using the `str()` function the
three variables in the data frame were summarized as "num" (i.e.,
numeric) or "chr" (i.e., character). R assumed that `integer_variable`
was a numeric variable type even though none of the values had decimal
places. It is often necessary to manually tell R what type of data each
variable represents. For example, we can specify that the `integer_variable`
is an integer type variable with the `as.integer()` function and that
the `character_variable` is a factor type variable with the
`as.factor()` function. 

We can replace the original variable in the dataframe with the new one, now assigned to its specific type, by using an object assignment approach. Alternatively, we could ask for R to create a new variable in the dataframe to represent the newly specified variable while retaining the original. You can also use the `class()` function to identify the type of data the variable represents (or the "type" of any object in R, for that matter).

For example:

```{r view_dataframe_specify, exercise = TRUE, exercise.eval = FALSE}
# Again, these first lines are just creating the dataframe for this example.
numeric_variable <- c(1.2, 3.7, 5, 2.2, 4.69, 6.123, 10.1, 4.3, 5.5, 6.09)
integer_variable <- c(4, 8, 15, 16, 23, 42, 12, 34, 78, 2)
character_variable <- c("who", "what", "when", "who", "what", "when", "who", 
                        "what",  "who", "what")
my_data <- data.frame(numeric_variable, integer_variable, character_variable)

# Assign the newly specified variable to the same name as the original to replace it.
my_data$integer_variable <- as.integer(my_data$integer_variable)

# Assign the newly specified variable to a new name to keep the original too.
my_data$character_variable_now_factor <- as.factor(my_data$character_variable)

# View the structure of the data again to see the newly specified variables.
str(my_data)
```


## *Functions and Arguments*

R comes "preloaded" with many, many functions. That is what is referred
to as "base" R. 

A function is essentially a self-contained bundle of code stored in a single name that can be used to execute the whole bundle on a given object (or objects) at once. For example, the `data.frame()`, `head()`, `tail()`, and `str()` functions used in the previous section take a dataframe object and execute some length of code to describe the contents of that dataframe object. In contrast, we used the `as.integer()` function above to take a single vector object and execute some length of code to coerce a numeric object into an integer class object.

The elements that are put inside the parentheses of a function are called **arguments**, which are specifications for options regarding how a given function is used. There are at least three easy way to learn about the specific arguments possible for each function:

-   Calling the name of the function preceded by a `?`, such as `?function.name()`
-   Looking up the function's description in package documentation on the official Comprehensive R Network website ([CRAN](https://cran.r-project.org/web/packages/available_packages_by_date.html))
-   Searching the "function name" and "R" on the web and looking for the many, many ways individual users have explained how to use the function (the R community of users is extremely generous and helpful when it comes to learning how to use R)

All functions come with default settings for its arguments and although in most cases those settings will be exactly what you need, you will eventually need to adjust certain arguments to avoid errors in your code. For example, you can calculate the arithmetic mean of a variable using the `mean()` function available in the base setup of R. Assuming you have no missing data in your variable, calling `mean(variable)` will work just fine. However, if there are missing values, then the function will report an error. That's because the `mean()` function has three arguments and one of them assumes that there are no missing values in the object you are using it on (see [mean documentation](https://rdrr.io/r/base/mean.html)). 

The arguments for `mean()` are:

-   `x`       the name of the object to calculate the mean of
-   `trim`    an argument that allows you to "trim" a fraction of the values from each end of the range of the variable prior to calculating the mean
-   `na.rm`   an argument that specifies whether missing values (labeled as "NA's") should be removed prior to calculating the mean or not — which can be specified with `na.rm = TRUE` or `na.rm = FALSE`

Importantly, the default `na.rm` argument setting in the `mean()` function is `na.rm = FALSE`. Therefore, if you are calling the `mean()` function with a variable that has missing values (i.e., "NA's"),  you would have to specify that by setting `mean(variable, na.rm = TRUE)`. Otherwise, you will get an error when trying to use the function.

Even very experienced R users do not remember all of the various options that are possible for the various arguments within functions. Calling help on any function `?function.name()` to review the specific argument possibilities available is a common activity for most users. Searching for examples of how to use functions online, either on pages of official documentation or otherwise, is also extremely common!

You could create your own functions in R fairly easily as well. As you gain experience with the programming language, building your own custom functions to enact common or repetitive tasks can save you a lot of time and typing.

## *Installing and Loading Packages*

You can easily expand the functionality of R by installing and using extension packages made by developers within the open-source R community. For beginner R users, this can be somewhat confusing since the combination of base functions with the possibilities of functions from additional packages can offer many ways for conducting any given procedure in R. Often, however, additional packages can make procedures in R more user-friendly. All packages that are available for installing using the typical methods within R are documented on the [R Website](https://cran.r-project.org/).

Here is an example of downloading a package and loading it for use in R.
The
[`rstatix`](https://cran.r-project.org/web/packages/rstatix/index.html)
package was designed by Alboukadel Kassambara to make some of the most
common statistical analyses that social scientists use simple to execute
in R.

```{r install_load_package, exercise = TRUE, exercise.eval = FALSE}
# Using the base R "install.packages()" function will signal to R to find 
# the package online, download it, and install it for use in your R system.
install.packages("rstatix")

# To "load" the package, use the base R "library()" function, then all of the 
# functions within the package # # will be available to you. You only need to 
# load a package once during an R session.
library(rstatix)
```

As an example of the convenience and functionality that packages offer,
consider that it is possible to calculate descriptive statistics for a
given variable in R using base functions such as: `mean()`, `sd()`,
`range()`, `min()`, `max()`, `median()`, and so on. But, there are more
convenient ways to get all of those summary statistics and more with
only one function from other packages. For example, both the **rstatix**
package and the `psych` package contain a single function for returning
all of those summary values.

```{r base_rstatix, exercise = TRUE, exercise.eval = FALSE}
# Again, these first lines are just creating the dataframe for this example.
numeric_variable <- c(1.2, 3.7, 5, 2.2, 4.69, 6.123, 10.1, 4.3, 5.5, 6.09)
integer_variable <- c(4, 8, 15, 16, 23, 42, 12, 34, 78, 2)
character_variable <- c("who", "what", "when", "who", "what", "when", "who", 
                        "what",  "who", "what")
my_data <- data.frame(numeric_variable, integer_variable, character_variable)

# Base R functions for summary statistics
mean(my_data$numeric_variable)
sd(my_data$numeric_variable)
range(my_data$numeric_variable)
min(my_data$numeric_variable)
max(my_data$numeric_variable)
median(my_data$numeric_variable)

# First "load" the package
library(rstatix)

# Then use the "get_summary_stats()" function to get a variety of summary statistics.
get_summary_stats(my_data, numeric_variable)
```

### *Tidyverse*

The [**tidyverse**](https://www.tidyverse.org/) refers to a suite of 8
packages created by a number of developers led by Hadley Wickham that
are designed to make various data science tasks more intuitive to do in
R. This tutorial will incorporate tidyverse-style coding because it can
ease the learning curve when it comes to getting started in R. Some
tidyverse packages we will use include **ggplot2**: a graphing package,
**dplyr**: a data manipulation package, and **readr** as well as
**haven**: packages for importing data. However, we will not have to
install and load the packages separately as installing and loading
**tidyverse** will set you up with all of them at once.

The **rstatix** package demonstrated above is designed to work
seamlessly with other packages in the tidyverse as well.

### *lavaan*

The [**lavaan**](https://lavaan.ugent.be/) package was designed by Yves
Rosseel for anlaysis tasks such as path analysis, confirmatory factor
analysis, structural equation modeling, and growth curve models. We will
use functions from the **lavaan** package when conducting factor
analyses.

## Importing and Viewing Various Types of Data Files

Conducting data analyses in R requires loading your data set (i.e., as
an object) into the R Global Environment. Unlike some programs such as
SPSS or Excel, there is not a direct data editor associated with R or R
Studio. However, there are packages with functions available for reading
virtually any type of data file into R.

Four common types of data files you are likely to encounter:

-   CSV Files (XXX.csv): "Comma-separated-value" data that can be
    created in a variety of programs
-   Excel Files (XXX.xlsx): Files create in Excel
-   SPSS Files (XXX.sav): Data editor files created in SPSS (which can
    contain a good deal of meta-data, e.g., variable names, value
    labels, and so on)
-   Stata Files (XXX.dta): Data files created in Stata (which can also
    contain m eta-data)

The [**readr**](https://readr.tidyverse.org/) package includes a
function for importing csv files: `read_csv()`. The
[**readxl**](https://readxl.tidyverse.org/) package has a function for
important Excel files: `read_xls()`. The
[**haven**](https://haven.tidyverse.org/) package includes functions for
importing SPSS files: `read_sav()`, and Stata files: `read_dta()`.

As implied in the bullet list above, some types of data files can be
more complex than others. For example, "XXX.csv" files will tend to be
very simple, whereas Excel files can have multiple sheets and SPSS or
Stata files can come with user-added meta-data. As such, the various
functions for reading in different types of data files have many built
in options for telling R how to interpret the file. We won't deal with
those here, suffice to say it's important to know what's in the file you
are reading into R and then checking that it came in the way you wanted
it to.

The following code demonstrates using functions in the readr package to
load each of the data types described above. The data are files included
as a supplement to the Miksza & Elpus text, [*Design and Analysis for
Quantitative Research in Music
Education*](https://www.amazon.com/Design-Analysis-Quantitative-Research-Education/dp/0199391904)
(the data is used in Chapter 4: Descriptive Analysis).

Note that this code won't actually run, because importing external files
into this tutorial program is too hard for me to figure out how to do.

```{r import_data, eval = FALSE}
# First we will load the packages with the necessary importing functions
library(readr)
library(haven)

# Assuming you are working within an R project and that your data files are in
# the same folder as your project files, importing the data can require very little code.
data_csv <- read_csv("Miksza & Elpus - Chorister data for descriptive analysis examples.csv")
data_SPSS <- read_sav("Miksza & Elpus - Chorister data for descriptive analysis examples.sav")
data_Stata <- read_dta("Miksza & Elpus - Chorister data for descriptive analysis examples.dta")

# Once imported you could use any number of functions to view the data, 
# e.g, "head()," "tail()," "str()," "glimpse()."
str(data_csv)
```

## Selecting, Filtering, and Altering Variables within Dataframes

In this section, we'll begin to use functions from the **tidyverse**
package to demonstrate how to select variables for analysis, filter data
according to specific values of a variable, and how to create new
variables. More specifically, we we will use the `select()`, `filter()`,
and `mutate()` functions from within the **dplyr** package. Given that
these functions are from a package within the tidyverse ecosystem, they
each can be used with what is referred to as a "*pipe operator*": `%>%`.
The pipe operator allows you to write code that can be more intuitive to
interpret than that written with some approaches in R base code.

For these exercises, we'll use a dataset I've provided with this
tutorial package called `DescriptiveData`. This dataset contains four
variables meant to represent characteristics of hypothetical singers in
a choir: (a) age — a numeric variable (i.e., ratio measure), (b) years
of experience in singing ensembles - an integer variable (i.e., ratio
measure), (c) self-reported biological sex at birth - a two-level factor
variable (i.e., nominal measure), and (d) rating of their preferred
frequency of rehearsing - a five-level ordered factor variable (i.e.,
ordinal measure). This particular dataset is included as a supplement to
the Miksza & Elpus text, [*Design and Analysis for Quantitative Research
in Music
Education*](https://www.amazon.com/Design-Analysis-Quantitative-Research-Education/dp/0199391904),
and is used to illustrate analyses in "Chapter 4: Descriptive Analysis."

The first six rows of the data are shown below:

```{r show descriptive data, exercise = FALSE, exercise.eval = FALSE, echo = FALSE}
head(DescriptiveData) %>% kable(align = "c")
```

The following code chunks will demonstrate how to select, filter, and
create variables using these data. There is also space for you to try
each function out yourself.

### *Selecting Variables*

```{r select, exercise = TRUE, exercise.eval = FALSE}
# First we start with the dataframe object then use the pipe to indicate that we
# want to pass the data to another  function, then we call the name of the 
# variable we want to select within the "select()" function, and then we can call a 
# function that we would like to use to act upon the selected variable - note 
# that you don't need to actually write the name of the variable in the final function.

# This is an example of selecting the "Age" variable and then identifying the
# minimum age value (i.e., the youngest age).
DescriptiveData %>% select(Age) %>% min()
```

Use the code space below to try out the `select()` function and the
`max()` function to identify the largest number of years of experience
in a singing ensemble among the participants:

```{r select_user, exercise = TRUE, exercise.eval = FALSE}

```

### *Filtering Data*

```{r filter, exercise = TRUE, exercise.eval = FALSE}
# Again, we start with the dataframe object then use the pipe to indicate that we
# want to pass the data to the following function, then we call the name of the 
# variable we want to use to to filter our data within the "filter()" function. 

# You can then continue with another pipe to act upon the filtered data with 
# additional functions, or you could create a new dataset from the filtered data 
# assigning the entire line of code to a newly named object.

# Here is an example of filtering the data so that only those who are 40 years or
# older remain, and then a call of the count() function to show how many of the 
# original 115 participants are left in the newly filtered data.
DescriptiveData %>% filter(Age >= 40) %>% count()

# Here is what it would look like to filter the data as above and then to assign
# the newly filtered data to a new dataframe object
DescriptiveData_Filtered <- DescriptiveData %>% filter(Age >= 40)

# You could use the "head()," "tail()," "glimpse()," or "str()" functions to view
# the new dataframe titled "DescriptiveData_Filtered." (That's not included here
# for the sake of saving space).

# Here is an example of filtering the data so that only those indicating a biological
# sex as Male are in the newly filtered data - note that quirks in this code
# include that the "equals" operator requires two signs, that when you work
# with levels of factor variables, they need to be presented in quotes, and
# that the variable names and category labels are case-sensitive.
DescriptiveData %>% filter(Sex == "Male") %>% count()
```

Use the code space below to try out the `filter()` function and the
`count()` functionto filter the data so that only those younger than Age
40 are left in the data, then try to filter the data so that only those
indicating a biological sex as Female are in the newly filtered data:

```{r filter_user, exercise = TRUE, exercise.eval = FALSE}

```

### *Creating New Variables with* `mutate()`

```{r mutate, exercise = TRUE, exercise.eval = FALSE}
# Once again, we start with the dataframe object then use the pipe to indicate that 
# we want to pass the data to the following function, but, with "mutate()" we need 
# to specify a name for our new variable and then a formula for how create the 
# new variable.

# The code below creates a new variable named "Age_Months" that is each participants' 
# original Age value multiplied by 12, then the "head()" function is called so that
# you can see that the new variable is appended to the dataframe.
DescriptiveData %>% mutate(Age_Months = Age*12) %>% head()
# You might have to use the arrow in the upper right of the output box to see
# all the columns, including the new one all the way to the right.
```

Use the code space below to try out the `mutate()` function and the
`head()` function to create a variable that is each participants' age in
days:

```{r mutate_user, exercise = TRUE, exercise.eval = FALSE}

```

## Basic Descriptive Analyses

### *Summary Statistics*

Creating frequency and crosstabs tables are perhaps two of the most
basic and fundamental tasks that analysts need to be able to do. The
base R functions `table()` and `proportions()` are good tools for these
tasks. The `table()` function will produce frequency counts in each
table cell whereas the `proportions()` function will produce cell
proportions.

```{r tables1, exercise = TRUE, exercise.eval = FALSE}
# Since "table()" is a base R function, we will not use the "pipe" as with 
# functions that work within the tidyverse. Instead, we'll use the dataframe and
# dollar sign approach to choose variables to analyze.

# Here is code to create a frequency table of the participants' preferred
# rehearsal frequency.
table(DescriptiveData$Preference.for.rehearsal.frequency)

# We can also assign the entire table to an object to make it easier to work
# with in other functions.
Rehearse_table <- table(DescriptiveData$Preference.for.rehearsal.frequency)

# The proportions function works with table objects.
proportions(Rehearse_table)

# If the number of decimal places is unwieldy, you can use the base R "round()"
# function to specify how many places you'd like to see.
round(proportions(Rehearse_table), 2)
```

Here is an example of a cross-tabulation using the same base R
functions.

```{r tables2, exercise = TRUE, exercise.eval = FALSE}
# Here is code to create a cross-tabulation table of the participants' preferred 
# rehearsal frequency according to biological sex.
table(DescriptiveData$Sex, DescriptiveData$Preference.for.rehearsal.frequency)

Bio_Rehearse_Table <- table(DescriptiveData$Sex, DescriptiveData$Preference.for.rehearsal.frequency)

round(proportions(Bio_Rehearse_Table), 2)
```

You could also request percentages *by row* by specifying an additional
argument within the `proportion()` function of "margin = 1" or *by
column* with "margin = 2".

```{r tables3, exercise = TRUE, exercise.eval = FALSE}
Bio_Rehearse_Table <- table(DescriptiveData$Sex, DescriptiveData$Preference.for.rehearsal.frequency)

round(proportions(Bio_Rehearse_Table, margin = 1), 2)

round(proportions(Bio_Rehearse_Table, margin = 2), 2)
```

As mentioned on earlier pages, the **rstatix** package provides many
functions that make conducting common types of analyses convenient
within a tidyverse ecosystem. The `freq_table()` function can be used in
a similar way as the base R `table()` function. The `freq_table()`
function returns frequency counts and percentages (even though a column
is printed that is labeled "prop").

```{r tables_rstatix1, exercise = TRUE, exercise.eval = FALSE}
# A line of code using a pipe operator can be used for this.
DescriptiveData %>% freq_table(Preference.for.rehearsal.frequency)
```

Here is an example of an **rstatix** cross-tabulation with the
`freq_table()` function.

```{r tables_rstatix2, exercise = TRUE, exercise.eval = FALSE}
# A line of code using a pipe operator can be used for this.
DescriptiveData %>% freq_table(Sex, Preference.for.rehearsal.frequency)
```

Examining descriptive statistics of quantitative variables is also a
fundamental step that all analysts will need to take. Such variables can
be summarized with the `get_summary_stats()` function from **rstatix**.

```{r summary_stat, exercise = TRUE, exercise.eval = FALSE}
# This code will provide summary statistics for the Age variable.
DescriptiveData %>% get_summary_stats(Age)

# There are additional arguments you could specify if you would like less 
# information. For example, adding a "type = common" argument to the function 
# would yield fewer summary statistics.
DescriptiveData %>% get_summary_stats(Age, type = "common")
```

### *Summary Statistics by Sub-Group*

It is also often necessary to summarize data according to sub-groups.
The `group_by()` function from the **dplyr** package can be used for
that type of task.

```{r summary_stat_group, exercise = TRUE, exercise.eval = FALSE}
# This code will provide summary statistics for the Age variable broken out by
# participants' reported biological sex
DescriptiveData %>% group_by(Sex) %>% get_summary_stats(Age, type = "common")
```

### *Correlation*

Examining bivariate correlations between variables is also a common
analysis task. As with all things in R, there are many, many ways to
calculate correlation coefficients in R and a variety of packages with
correlation functions. The **rstatix** package has several useful
functions for calculating correlations. An additional element of
flexibility in all of the rstatix correlation functions is the ability
to specify among several common types of correlation: Pearson, Spearman,
or Kendall.

```{r correlate, exercise = TRUE, exercise.eval = FALSE}
# The "cor_test()" function is quite flexible and intuitive. This code returns
# a Pearson correlation between Age and Years experience in a singing ensemble.
DescriptiveData %>% cor_test(Age, Years.experience.in.singing.ensemble)

# There is also a useful function for creating correlation matrices "cor_mat()".
# Since there are only two interval/ratio level variables in this example
# dataset, it would be inappropriate to demonstrate a Pearson correlation matrix
# of more than two variables.

# It could also be helpful to be able to examine correlations according to
# sub-groups. This code first filters the Sex variable so only those who reported
# their biological sex are considered in the analyses, then it will result in 
# the Pearson correlation between Age and Years experience in a singing ensemble 
# broken out by participants' reported biological sex.
DescriptiveData %>% filter(Sex == "Male" | Sex == "Female") %>%
  group_by(Sex) %>% cor_test(Age, Years.experience.in.singing.ensemble)
```

## Basic Data Visualization with ggplot2

The graphic capabilities of R are amazing and your options for
visualizing data are vast. In this section, we'll again use the
`DescriptiveData` example dataframe to demonstrate how to make some of
the most common types of graphs. We will continue working within the
**tidyverse** ecosystem and use the extremely flexible package
**ggplot2**.

### *ggplot2*

The "gg" in **ggplot2** refers to the "grammar of graphics," which is a
reference to a book by Leland Wilkinson. The primary developer of
**tidyverse** packages and functions, Hadley Wickham, also developed
**ggplot2**. The most basic elements or "layers" of a plot rendered with
**ggplot2** include: (a) a dataframe, (b) the designation of
"aesthetics" (e.g., the mapping of variables to axes and aesthetic
features), and (c) a "geom" (e.g., a shape for the plot such as
histogram, boxplot, barchart, scatterplot, and so on).

From there, there are many options for building additional layers to the
plot and a tremendous amount of flexibility in determining the specific
look of the features of the plot. When it comes to making beautiful
plots, there are too many good books and tutorials online that you can
learn from. For our purposes in this section, we will just make basic
(albeit ugly) plots. Several basic options for customizing plots will be
demonstrated in the next section.

The most basic code for plot would look something like this:

-   First call the function `ggplot()` and specify the data source:
    -   `ggplot(data = your_data)`
-   Next, specify which variables from the dataframe are assigned to
    which axes using the `aes()` function inside of the `ggplot()`
    function:
    -   `ggplot(data = your_data, aes(x = variable_1))`
    -   (There are many more aesthetic possibilities beyond designating
        axes)
-   Next, choose a "geom" to indicate what type of plot you are aiming
    to make by adding one of the possible options with a plus sign (`+`)
    after the `ggplot()` function:
    +`ggplot(data = your_data, aes(x = variable_1)) + geom_histogram()`

### *Histogram*

Creating a histograms is critical for examining the distribution of a
quantitative variable. Here is an example of code for creating a
histogram of the Age variable from the example `DescriptiveData` dataset
that follows the generic form described above.

```{r histogram, exercise = TRUE, exercise.eval = FALSE, message = FALSE}
# Note that the "data =" part of the code was removed since the ggplot() function
# will assume the first argument is the name of the dataframe.
ggplot(DescriptiveData, aes(x = Age)) + geom_histogram()
```

Use the code space below to try and code a histogram for the Years
experience in a singing ensemble variable:

```{r histogram_user, exercise = TRUE, exercise.eval = FALSE, message = FALSE}

```

### *Boxplot*

Creating a boxplot instead is simply a matter of choosing a different
geom. Here is an example of code for creating a boxplot of the Age
variable. Note that the values on the y axis are not informative and refer to the width of the box. You'll see how to remove them in the next section of this tutorial.


```{r boxplot1, exercise = TRUE, exercise.eval = FALSE}
ggplot(DescriptiveData, aes(x = Age)) + geom_boxplot()
```

You could make it a more typical looking vertical-oriented boxplot by
assiging the Age variable to y instead of x.

```{r boxplot2, exercise = TRUE, exercise.eval = FALSE}
ggplot(DescriptiveData, aes(y = Age)) + geom_boxplot()
```

Use the code space below to try and code a boxplot for the Years
experience in a singing ensemble variable:

```{r boxplot_user, exercise = TRUE, exercise.eval = FALSE}

```

### *Barplot*

Creating barplots for nominal (i.e., factor in R) or ordinal variables
(i.e., ordered factor in R) can be done with very similar code.

```{r barchart, exercise = TRUE, exercise.eval = FALSE}
ggplot(DescriptiveData, aes(x = Preference.for.rehearsal.frequency)) + geom_bar()
```

Creating "stacked" or "dodged" barplots for two nominal (i.e., factor in
R) or ordinal variables (i.e., ordered factor in R) would require
assigning a second variable to `fill = variable_2` in the `aes()`
function and then specifying an additional argument of
`position = "stack"` or `position = "dodge"` within the `geom_bar()`
function.

```{r barchart_dodge, exercise = TRUE, exercise.eval = FALSE, message =FALSE}
ggplot(DescriptiveData, aes(x = Preference.for.rehearsal.frequency,
                            fill = Sex)) + geom_bar(position = "dodge")
# Note that this plot is particularly odd since there are "empty cells" for 
# some combinations of these two variables.
```

### *Scatterplot*

Creating scatterplots to depict the relationship between two
quantitative variables will feel similarly intuitive once you get the
hang of the grammar of graphics approach. Here is a scatterplot of the
Age and Years experience in a singing ensemble variables.

```{r scatterplot1, exercise = TRUE, exercise.eval = FALSE, message = FALSE}
ggplot(DescriptiveData, aes(x = Age,
                            y = Years.experience.in.singing.ensemble)) + geom_point()
```

Adding a layer consisting of a line of best fit and the associated
standard error can be done by adding another layer with `geom_smooth()`
along with an argument specifying a linear model: \`method = "lm".
(Clearly this is a very weak/non-existent correlation!)

```{r scatterplot2, exercise = TRUE, exercise.eval = FALSE, message = FALSE}
ggplot(DescriptiveData, aes(x = Age,
                            y = Years.experience.in.singing.ensemble)) + 
  geom_point() +
  geom_smooth(method = "lm")
```

## A Bit Less Basic Data Visualization with ggplot

In this section, a few of the additional options for customizing plots
in ggplot2 will be demonstrated. We will use a different dataset that is
built into this package for these demonstrations since it is larger and can 
allow for few more options. It is titled `InferentialTestData`, 

This dataset contains nine variables representing characteristics and
measurements of school children: (a) kindergarten students'
standardized math test scores — a numeric variable (i.e., interval
measure), (b) those same students' 1st grade standardized math test
scores — a numeric variable (i.e., interval measure), (c) the urbanicity
of the students' school (i.e., city, suburb, town, rural) - a factor variable 
(i.e., nominal measure), (d) whether they participated in school music class 
(i.e., No Music, Yes Music) - a factor variables (i.e., a nominal variable), and
(e) a set of "dummy" variables that indicate "Yes" or "No" for each urbanicity 
category - factor variables. This particular dataset is included as a supplement 
to the Miksza & Elpus text, [*Design and Analysis for Quantitative Research in Music
Education*](https://www.amazon.com/Design-Analysis-Quantitative-Research-Education/dp/0199391904), and is used to illustrate analyses in "Chapter 10: Regression."

The first six rows of the data are shown below:

```{r show inferentialTest data, exercise = FALSE, exercise.eval = FALSE, echo = FALSE}
head(InferentialTestData) %>% kable(align = "c")
```

### *Additional Common Plot Customization*

First, we will consider methods for enacting several very common desirable 
alterations to default plots generated by *ggplot2*. Then, we will create some plots incorporating these methods. Please be aware that these tips barely scratch the
surface of the customizations available in the *ggplot2* package. Fortunately, 
there are many, many terrific online resources for making virtually any sort of
alteration you'd like.

#### *Additional Aesthetic Mappings*
All primary elements of a plot can be customized according to how you would like
them to appear. For example, within the `aes()` function it is possible to specify
outline and fill colors, point size, point shape, line thickness, linetype, 
degree of transparency, geom dimensions, and so on. Various aesthetic mappings will be demonstrated in the sample code presented below.

#### *Axis Labels*

By default, *ggplot* will use variable names as axis label and will not include
any additional title or sub-title information. Adding custom labels and titles can
be done by adding an additional function to the plot code of `+` `labs()` and
specifying labels in quotes for elements of the plot such as: `x = `, `y = `, 
`title = `, `subtitle = `, or `caption = `.

#### *Adjusting Axis Ranges and Ticks*

Sometimes when plotting quantitative variables, the default plot will produce 
axes and indicators or "ticks" of values that make it challenging to interpret 
the range of values in a plot. You can customize x and y axes in *ggplot* by adding additional functions such `+` `scale_x_continuous()` and `+` `scale_x_continuous()`. 
Then specifying the range of values to be included by inserting the function `limits =`
into the parentheses of the previous functions, followed by a vector of two values representing the desired minimum and maximum, such as, `+` 
`scale_x_continuous(limits = c(1, 100))` - that would create an x axis bound by
1 and 100.

If, say, you'd like to customize the axis ticks that are shown between the axis 
minimum and maximum values, then you could also insert the function `breaks = ` into the parentheses of the `scale_x_continuous()` function, followed by a vector of values
specifying each tick point. For example, the following code would produce an x axis
with a range of 1 to 100 and ticks at every multiple of 10: 
`scale_x_continuous(limits = c(1, 100), breaks = c(10, 20, 30, 40, 50, 60, 70, 80, 90, 100))`.

There are also similar methods available for customizing axes representing all 
sorts of variable types, not just quantitative variables.

#### *Theme Elements*

The general "look" of a plot can be controlled with a number of functions that 
can be inserted within a `+` `theme()` layer. Take a look at the online 
documentation of the various elements that can be adjusted within a theme layer 
[Theme Element Options](https://ggplot2.tidyverse.org/reference/theme.html). 
The options are certainly numerous enough that they can be overwhelming when getting started. 

However, *ggplot2* also has several complete themes that you could use which 
convert many elements of a plot to a particular style. You can see some of the 
options for complete themes at the *tidyverse* website as well: [Complete Theme Options](https://ggplot2.tidyverse.org/reference/ggtheme.html). The name of the
the complete theme is typically appended to the `theme()` function. For example, 
to use what is referred to as a minimalist style in *ggplot2*  you could add a 
layer that looks something like: `+` `theme_minimal()`.

### *Histogram*

Here is code for a histogram depicting the distribution of the students' 1st grade standardized math test score variable from the `InferentialTestData` data set.

```{r histogramDetail1, exercise = TRUE, exercise.eval = FALSE, message = FALSE}
ggplot(InferentialTestData, aes(x = mathtest1)) + geom_histogram()
```

Here is code for the same histogram, but this time incorporating the features 
described above. Note that you can continue adding layers to the plot code on a 
new line if the previous line ends with a `+` sign.

```{r histogram_userDetail2, exercise = TRUE, exercise.eval = FALSE, message = FALSE}
ggplot(InferentialTestData, aes(x = mathtest1)) + 
  geom_histogram(color = "white", fill = "blue") +
  labs(title = "Distribution of 1st Grade Students' Standardized Math Test Scores",
       x = "Math Test Score", y = "Frequency") +
  scale_y_continuous(limits = c(0, 100), breaks = c(20, 40, 60, 80, 100)) +
  theme_minimal()
```

Use this code space to customize your own histogram of math test scores.

```{r histogramDetail_user, exercise = TRUE, exercise.eval = FALSE, message = FALSE}
```

### *Boxplot*
Here is code for a vertical boxplot depicting the distribution of the students' 
1st grade standardized math test score variable from the `InferentialTestData` 
data set.

```{r boxplot1Detail1, exercise = TRUE, exercise.eval = FALSE}
ggplot(InferentialTestData, aes(y = mathtest1)) + geom_boxplot()
```

Here is code for the same boxplot, but this time incorporating the features 
described above with customizations. We will add the traditional "whiskers" to 
this version with the additional argument of `staplewidth = ` within the 
`geom_boxplot()` function and eliminate the nuisance information on the x axis 
by specifying `breaks = NULL` in the `scale_x_continuous()` function.

```{r boxplot1Detail2, exercise = TRUE, exercise.eval = FALSE}
ggplot(InferentialTestData, aes(y = mathtest1)) + 
  geom_boxplot(fill = "lightblue", staplewidth = .25) +
  labs(title = "Distribution of 1st Grade Students' Standardized Math Test Scores",
       y = "Math Test Score") +
  scale_x_continuous(breaks = NULL) +
  scale_y_continuous(limits = c(0, 100), breaks = c(0, 15, 30, 45, 60, 75, 90)) +
  theme_classic()
```

Use this code space to customize your own boxplot of math test scores.

```{r boxplot_userDetail, exercise = TRUE, exercise.eval = FALSE}
```

### *Barplot*

Here is code for a barplot depicting the students' urbanicity variable from the `InferentialTestData` data set. Note that the labels for each bar are taken 
directly from the factor variable levels. You can change those by relabeling the
levels of factor variable outside of the `ggplot()` function using the `labels = `
argument in the `factor()` function.

```{r barchartDetail, exercise = TRUE, exercise.eval = FALSE}
ggplot(InferentialTestData, aes(x = urbanicity)) + geom_bar()
```

Here is code for a dodged barplot depicting the frequency of students in each
urbanicity category according to whether they are in school music or not. Note 
that the title and labels for the legend that is produced are drawn from the name
of the music factor variable and names of its levels. These can be adjusted by 
renaming the variable and relabeling the factor levels.

```{r barchart_crosstabDetail1, exercise = TRUE, exercise.eval = FALSE}
ggplot(InferentialTestData, aes(x = urbanicity, fill = music)) + 
  geom_bar(position = "dodge")
```

Here is code for the same barplot with additional customizations. Note that with
two variables, it is necessary to make changes in color with more specific code.

```{r barchart_crosstabDetail2, exercise = TRUE, exercise.eval = FALSE}
ggplot(InferentialTestData, aes(x = urbanicity, fill = music)) + 
  geom_bar(position = position_dodge(width = 1), width = .8) +
  scale_fill_manual(values = c("red", "blue")) +
  labs(x = "Urbanicity", y = "Frequency Count",
       title = "Cross-tabulation of Urbanicity and School Music Participation",
       subtitle = "This is an example of a sub-title.") +
  theme_minimal()
```

Use this code space to customize your own dodged barplot.

```{r barchart_crosstabDetail_user, exercise = TRUE, exercise.eval = FALSE}
```

### *Scatterplot*

Here is code for a scatterplot depicting the relationship between the students'
kindergarten and 1st grade standardized math test scores.

```{r scatterplot1Detail1, exercise = TRUE, exercise.eval = FALSE, message = FALSE}
ggplot(InferentialTestData, aes(x = mathtestk, y = mathtest1)) + 
  geom_point()
```

Here is code for the same scatterplot with customizations. Note that the 
`alpha = ` argument is used in the `geom_point()` function to add a degree of
transparency to the points so that the overplotting (e.g., lots of points in the 
same general area) is a bit easier to see. Also, since this is the final example, 
it demonstrates including a variety of specific theme specifications rather than
a complete theme.

```{r scatterplot1Detail2, exercise = TRUE, exercise.eval = FALSE, message = FALSE}
ggplot(InferentialTestData, aes(x = mathtestk, y = mathtest1)) + 
  geom_point(color = "blue", alpha = .35) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(x = "Kindergarten Math Test", y = "1st Grade Math Test",
       title = "Scatterplot of Participants' Kindergarten and 1st Grade Test Scores") +
  scale_x_continuous(limits = c(5, 85), 
                     breaks = c(10, 20, 30, 40 , 50, 60, 70, 80)) +
  scale_y_continuous(limits = c(5, 85),
                     breaks = c(10, 20, 30, 40 , 50, 60, 70, 80)) +
  theme(title = element_text(size = 15),
        axis.line = element_blank(),
        axis.text = element_text(size = 10),
        axis.title = element_text(size = 13),
        panel.grid.minor = element_line(color = "lightgrey"),
        panel.grid.major = element_line(color = "grey"),
        panel.background = element_rect(fill = NA),
        axis.ticks = element_blank())
```

Use this code space to customize your own scatterplot.

```{r scatterplotDetail_user, exercise = TRUE, exercise.eval = FALSE}

```

## Basic Inferential Analyses and Formula Notation

We will continue to use the `InferentialTestData` dataset described in the previous section for the examples in this section. To keep things relatively simple, we will also restrict our analyses to the use of the base R linear model (i.e., `lm()`) function and other functions available in the **rstatix** package described earlier. In addition, all of the functions that will be demonstrated in the remaining sections can take a formula as the argument for specifying the test you would like to conduct. As such, we'll begin with a quick introduction to formula notation.

### *Using Formula Notation*

For the analyses demonstrated in the next section, the formula notation will involve specifying an dependent variable (i.e., an outcome, `y`) and independent variables (i.e., predictors, `x`). The tilde, `~`, symbol is used to separate the dependent from the independent variables. Therefore, a basic analysis of the influence of one independent variabl on one dependent variable would look like this:

|   `y ~ x`
\
For analyses with more than one independent variable, a plus sign, `+`, can be used to extend the formula. If interaction effects are desired between independent variables, then a colon, `:`, could be used between them. Therefore, an analysis involving two independent variables (or an independent variable and a covariate, etc.) could look like this:

|   `y ~ x1 + x2`
\
The interaction effect could be added like this:

|   `y ~ x1 + x2 + x1:x2`

### *A Quick Note About Options Available in R*

It's worth taking a moment to mention that in addition to base R functions, there are many packages that have functions for doing the relatively simple analyses presented in the sections below. This note is a reminder that all of the test functions shown here are from the **rstatix** package.

Also recall that the **rstatix** package is designed to work well with the **tidyverse** suite of packages. As such, we will make frequent use of the pipe operator, `%>%` to pass the `InferentialTestData` dataframe to various tests functions.

## Linear Regression Models

Many of the statistical tests arts education researchers wish to use can be executed with some version or generalized form of the **general linear model**. For example, correlation, t-tests, ANOVAs, and regression analyses can all be interpreted as an instantiation of the general linear model. For example, all of the objectives of the statistical tests mentioned in the previous sentence can be achieved with the single base R function `lm()`, which is for fitting linear models.

The `lm()` function is quite simple to use. It involves specifying an argument that is a formula with variable names to indicate dependent and independent variables (e.g., `outcome_var ~ predictor_var`) and then specifying an arugment indicating the dataframe in which those variables can be found (e.g., `data = your_dataframe`). 

### *Simple Linear Regression*

#### *Interval/Ratio Predictor Variables*

Here is an example of specifying a linear model that analyzes the relationship between participants' kindergarten and 1st grade standardized math test scores. Note that it is helpful to assign the results of an `lm()` analysis to its own object. That way, you can call the base R `summary()` function on the `lm()` object, which will yield an informative results output.

With two numeric variables, this is essentially the same as conducting a Pearson correlation. In fact, calling for the "standardized" regression coefficient for this analysis will result in the Pearson coefficient. There is a function for standardized regression coefficients in the **QuantPsyc** package called `lm.beta()` that can be used for this.

```{r regression_cor, exercise = TRUE, exercise.eval = FALSE, message = FALSE}
# First, specify the model and assign it to an object with a name you make up.
kind_1st_mod <- lm(mathtest1 ~ mathtestk, data = InferentialTestData)

# Next, call "summary()" to get the results.
summary(kind_1st_mod)
```

Evaluating the assumptions of a linear model is also a straightforward task in R. Calling the generic base R `plot()` function on a `lm()` object will result in a series of plots that can be used to assess the heterescedasticity and normality of residuals.

```{r regression_cor_assump, exercise = TRUE, exercise.eval = FALSE, message = FALSE}
# First, specify the model and assign it to an object with a name you make up.
kind_1st_mod <- lm(mathtest1 ~ mathtestk, data = InferentialTestData)

# Use the "plot()" function to evaluate model assumptions
plot(kind_1st_mod)
```

For illustration purposes, here is code calling the "lm.beta()" function from the "QuantPsyc" package on the model to get the standardized coefficient. This is followed by code for the Pearson correlation for confirmation of the equivalence.

```{r regression_cor_stand, exercise = TRUE, exercise.eval = FALSE, message = FALSE}
# Getting the standardized coefficient
lm.beta(lm(mathtest1 ~ mathtestk, data = InferentialTestData))

# Pearson correlation
InferentialTestData %>% cor_test(mathtest1, mathtestk)
```

#### *Nominal/Ordinal Predictor Variables*

Assuming the variables in a dataframe are appropriately classified according to their measurement scale (e.g., numeric, factor, ordered factor), then R will interpret them appropriately in the within `lm()` function. Therefore, using nominal or ordinal predictor variables will result in R automatically creating k-1 dummies with the 1st level of the factor as the reference category for such variables. There are fairly simple ways to change the reference category and the general approach to factor variables in `lm()` from the default, but we won't cover those in this tutorial.

Here is an example of specifying a linear model that analyzes whether participants'  kindergarten standardized math test scores varies as a function of the urbanicity of their school. Note that the reference category for the k-1 urbanicity dummy variables is the "city" category.

With an interval/ratio-level outcome variable and a more than 2-category nominal predictor variable, this is essentially the same as conducting an ANOVA. We will use the same variables to demonstrate a oneway ANOVA in the next section. 

```{r regression_nom, exercise = TRUE, exercise.eval = FALSE, message = FALSE}
# First, specify the model and assign it to an object with a name you make up.
mathk_urb_mod <- lm(mathtestk ~ urbanicity, data = InferentialTestData)

# Next, call "summary()" to get the results.
summary(mathk_urb_mod)
```

You can actually call the base R `anova()` function on an `lm()` object to get the traditional ANOVA table as output rather than regression results.

```{r regression_nom_anova, exercise = TRUE, exercise.eval = FALSE, message = FALSE}
# First, specify the model and assign it to an object with a name you make up.
mathk_urb_mod <- lm(mathtestk ~ urbanicity, data = InferentialTestData)

# Next, call "anova()" to get an ANOVA table.
anova(mathk_urb_mod)
```

### *Multiple Linear Regression*

Adding multiple predictor variables/covariates or higher order terms (e.g., a squared term for quadratic effects) is simply a matter of continuing the formula argument with plus signs `+` and additional variable names. As mentioned above including interaction terms can be done with the `:` symbol between variable names, as in `variable1:variable2`.

Here is code for a model that regresses both, whether participants participated in music and their socioeconomic status on their 1st grade standardized math test scores.

```{r regression_mult, exercise = TRUE, exercise.eval = FALSE, message = FALSE}
# First, specify the model and assign it to an object with a name you make up.
math1_mult_mod <- lm(mathtest1 ~ music + ses, data = InferentialTestData)

# Next, call "summary()" to get the results.
summary(math1_mult_mod)
```

#### *Model Comparison*

Model comparisons can be conducted by assigning the models you would like to compare to separate objects (e.g., `model_1`, `model_2`), and then calling the `anova()` function with the two model names as the arguments (e.g., `anova(model_1, model_2)`).

```{r regression_mult_compare, exercise = TRUE, exercise.eval = FALSE, message = FALSE}
# First specify the single predictor model.
math1_mus_mod <- lm(mathtest1 ~ music, data = InferentialTestData)

# Next specify the multiple predictor model.
math1_mult_mod <- lm(mathtest1 ~ music + ses, data = InferentialTestData)

# Finally, call "anova()" function to compare the two models.
anova(math1_mus_mod, math1_mult_mod)
```

## Traditional Tests for Group Comparisons

### *Comparing Two Independent Samples*

The following code uses the `t_test()` function to compare 1st grade standardized math test scores according to whether the participants were in music or not. 
```{r independent_t, exercise = TRUE, exercise.eval = FALSE}
InferentialTestData %>% t_test(mathtest1 ~ music)
```

If the variances of our groups were not equal, we could adjust the default setting of the `var.equal = ` argument within the `t_test()` function to be `var.equal = FALSE`. By doing that, the more robust Welch version of the t-test would be run.

If we needed to compare "dependent samples" instead, then we could adjust the `paired =` argument from its default setting of `FALSE` to `paired = TRUE`.

Use the code space below to test whether participants' socioeconomic status (i.e., a dependent variable in this case, `ses`) varies as a function of whether participants were in music or not (i.e., an independent variable, `music`).
```{r independent_tuser, exercise = TRUE, exercise.eval = FALSE}
```

### *Effect Size for Comparing Two Independent Samples*

Calculating the effect size for the difference between two independent means (i.e., Cohen's d, `cohens_d()`) involves similar code:
```{r cohen_d, exercise = TRUE, exercise.eval = FALSE}
InferentialTestData %>% cohens_d(mathtest1 ~ music)
```

The same changes to arguments within the `cohens_d()` argument as describe above can be made if the analyses involved independent samples with unequal variance (`var.equal = FALSE`) or paired samples (`paired = TRUE`). Similarly, an argument specifying `hedges.correction = TRUE` can be added if Hedge's G is preferred.

Use the code space below to determine the effect size of the difference in participants' socioeconomic status as a function of whether participants were in music or not.
```{r cohen_duser, exercise = TRUE, exercise.eval = FALSE}
```

### *Comparing More than Two Independent Samples*

Conducting a oneway ANOVA to compare more than two independent samples is also quite straightforward. Note that the effect size for this analysis, in this case generalized eta squared, is returned from this function by default.

```{r anova, exercise = TRUE, exercise.eval = FALSE}
InferentialTestData %>% anova_test(mathtestk ~ urbanicity)

```

Use the code space below to run a similar test, but with 1st grade standardized math test scores as the dependent variable.

```{r anova_user, exercise = TRUE, exercise.eval = FALSE}

```

Probing pairwise comparisons as a follow-up to ANOVA analyses can be done in several ways. Here we use the `tukey_hsd()` function. Note that this function also takes a formula and that the results returned are adjusted for multiple comparisons using the Tukey method.

```{r anovapost, exercise = TRUE, exercise.eval = FALSE}
InferentialTestData %>% tukey_hsd(mathtestk ~ urbanicity)

```

Alternatively the `pairwise_t_test()` function with an additional argument of `p.adjust.method =` set to one of the available options (`"bonferroni"`, `"hochberg"`, and so on) could be used.

### *Factorial Designs*

ANOVA models involving more than one independent variable (and interactions) can be estimated with the same `anova_test()` function used above, or with the `factorial_design()` function.

Here is an example of a test of whether 1st grade standardized math test scores vary as a function of music participation, urbanicity - or whether the effect of music participation depends upon students' urbanicity, that is, the interaction of music and urbanicity. 

(Also note that the argument `type = 3` was added to this code to indicate the analysis should use type 3 sums of squares. Type 3 sums of squares is a default in some commercial statistical packages such as SPSS.)

```{r anova2way, exercise = TRUE, exercise.eval = FALSE}
InferentialTestData %>% anova_test(mathtest1 ~ music + urbanicity + music:urbanicity, type = 3)

```

Further pairwise comparisons of significant effects can be conducted in a variety of ways. As one example, it might be desirable to see if urbanicity is a significant effect for those who do and those who do not participate in music. That can be done by first grouping according to music, and then conducting tests of estimated marginal means with **p** values adjusted for multiple comparisons. 

```{r emmeans, exercise = TRUE, exercise.eval = FALSE}
InferentialTestData %>% group_by(music) %>%
  emmeans_test(mathtest1 ~ urbanicity, p.adjust.method = "bonferroni")

```

### *Specifying Within-Subjects Factors*

All of the ANOVA examples so far have involved between-subjects factors, exclusively. Specifying models with within-subjects factors is a bit more complicated in R. 

### *Assumptions and Non-Parametric Analogs*

Typical tests of assumptions for the tests mentioned above are also available in the **rstatix** package and can be called with similar code. Here are a few among the options available:

* levene_test()
* shapiro_test()
* identify_outliers()

The non-parametric analogs of the t-test for nominal or ordinal outcome data are similarly accessible in the **rstatix** package. Use the `?function_name()` help call on the tests below to see what options are avaialble:

* `chisq_test()`
* `mcnemar_test()`
* `wilcox_test()`
* `kruskal_tes()`
* `friedman_test()`
